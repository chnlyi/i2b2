{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T18:59:25.540026Z",
     "start_time": "2019-06-19T18:59:24.949318Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas \n",
    "import glob\n",
    "import gensim\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Nadam\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import EarlyStopping\n",
    "# from keras.utils import plot_model  \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import Counter\n",
    "\n",
    "dictionary = pandas.read_csv(\"../../data/dictionary.txt\", delim_whitespace=True, header=None)\n",
    "dictionary = dictionary.set_index(1)[0].to_dict()\n",
    "classes = pandas.read_csv(\"../../data/classes.txt\", delim_whitespace=True, header=None)\n",
    "classes = classes.set_index(0)[1].to_dict()\n",
    "\n",
    "files = glob.glob(\"./data/training/*.txt\")\n",
    "training_set = [pandas.read_csv(file, delim_whitespace=True, header=None).values for file in files]\n",
    "\n",
    "train_records, train_labels = [], []\n",
    "X_train, Y_train = [], []\n",
    "\n",
    "for record in training_set:\n",
    "    train_records.append(numpy.array([[int(y) for y in x.split(',')] for x in record[:, 0]]))\n",
    "    train_labels.append(numpy.array([[int(y) for y in x.split(',')] for x in record[:, 1]]))\n",
    "\n",
    "for i in range(len(train_records)):\n",
    "    X_train.append([])\n",
    "    Y_train.append([])\n",
    "    for j in range(len(train_records[i])):\n",
    "        X_train[i].extend(train_records[i][j])\n",
    "        Y_train[i].extend(train_labels[i][j])\n",
    "    X_train[i] = numpy.array(X_train[i])\n",
    "    Y_train[i] = numpy.array(Y_train[i])\n",
    "\n",
    "nb_words = 36664\n",
    "max_length = 3390\n",
    "embedding_dim = 20\n",
    "\n",
    "y_train = [list(set(x)) for x in Y_train]\n",
    "y_train = [y for x in y_train for y in x]\n",
    "y_train = [x for x in y_train if x != 1]\n",
    "\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "Y_train = sequence.pad_sequences(Y_train, maxlen=max_length)\n",
    "\n",
    "Y_train = numpy.array([y for x in Y_train for y in x])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y_train)\n",
    "encoded_Y = encoder.transform(Y_train)\n",
    "encoded_Y = np_utils.to_categorical(encoded_Y)\n",
    "encoded_Y = numpy.array([encoded_Y[i:i + max_length] for i in range(0, len(encoded_Y), max_length)])\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec.load('../word2vec/word2vec.model')\n",
    "embedding_weights = numpy.zeros((nb_words, embedding_dim))\n",
    "\n",
    "for word, index in dictionary.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_weights[index,:] = word2vec_model[word]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(nb_words, embedding_dim, input_length=max_length, mask_zero=True, weights=[embedding_weights]))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(TimeDistributed(Dense(encoded_Y.shape[2], activation='softmax')))\n",
    "\n",
    "optimiser = Nadam(lr=0.004, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimiser) \n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True) \n",
    "\n",
    "print(model.summary())\n",
    "print(model.get_config())\n",
    "\n",
    "# early_stopping_monitor = EarlyStopping(monitor='loss', patience=5)\n",
    "# model.fit(X_train, encoded_Y, epochs=60, batch_size=32, callbacks=[early_stopping_monitor], verbose=2)\n",
    "model.fit(X_train, encoded_Y, epochs=40, batch_size=32, verbose=2)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"blstm-model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"blstm-model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n",
    "test_files = glob.glob(\"./data/test/gold/*.txt\")\n",
    "test_set = [(pandas.read_csv(x, delim_whitespace=True, header=None)).values for x in test_files]\n",
    "\n",
    "X_test = [[[int(z) for z in str(y[0]).split(',')] for y in x] for x in test_set]\n",
    "Y_test = [x[-1] for x in X_test]\n",
    "X_test = [x[0:-1] for x in X_test]\n",
    "Y_test = [x if x[0] != 0 else [] for x in Y_test]\n",
    "\n",
    "X_test = numpy.array(X_test)\n",
    "Y_test = numpy.array(Y_test)\n",
    "\n",
    "x_test, y_test = [], Y_test\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    x_test.append([])\n",
    "    for j in range(len(X_test[i])):\n",
    "        x_test[i].extend(X_test[i][j])\n",
    "    x_test[i] = numpy.array(x_test[i])\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "json_file = open('blstm-model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"blstm-model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=optimiser)\n",
    "\n",
    "predictions = loaded_model.predict(x_test)\n",
    "predictions = numpy.array([[[round(z) for z in y] for y in x] for x in predictions])\n",
    "predictions = [x.argmax(1) for x in predictions]\n",
    "predictions = [list(set(x)) for x in predictions]\n",
    "predictions = [[y for y in x if y != 0 and y != 1] for x in predictions]\n",
    "\n",
    "for i in range(len(test_files)):\n",
    "    prediction = [classes[x][2::] for x in predictions[i]]\n",
    "\n",
    "    file = open(\"output/\" + test_files[i][17:24] + \"xml\", 'w')\n",
    "    file.write(\"<?xml version='1.0' encoding='UTF-8'?>\\n\")\n",
    "    file.write(\"<root>\\n\")\n",
    "    file.write(\"\\t<TAGS>\\n\")\n",
    "\n",
    "    for label in prediction:\n",
    "        label = label.split('.')\n",
    "        if len(label) == 3:\n",
    "            if label[2] == 'continuing':\n",
    "                if label[0] == 'medication':\n",
    "                    element = label[0].upper()\n",
    "                    type1 = label[1].replace('_', ' ')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"before dct\" type1=\"' + type1 + '\" type2=\"\"/>\\n')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"during dct\" type1=\"' + type1 + '\" type2=\"\"/>\\n')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"after dct\" type1=\"' + type1 + '\" type2=\"\"/>\\n')\n",
    "                else:\n",
    "                    element = label[0].upper()\n",
    "                    indicator = label[1].replace('_', ' ')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"before dct\" indicator=\"' + indicator + '\"/>\\n')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"during dct\" indicator=\"' + indicator + '\"/>\\n')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"after dct\" indicator=\"' + indicator + '\"/>\\n')\n",
    "            else:\n",
    "                if label[0] == 'medication':\n",
    "                    element = label[0].upper()\n",
    "                    time = label[2].replace('_', ' ')\n",
    "                    type1 = label[1].replace('_', ' ')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"' + time + '\" type1=\"' + type1 + '\" type2=\"\"/>\\n')\n",
    "                else:\n",
    "                    element = label[0].upper()\n",
    "                    time = label[2].replace('_', ' ')\n",
    "                    indicator = label[1].replace('_', ' ')\n",
    "                    file.write('\\t\\t<' + element + ' time=\"' + time + '\" indicator=\"' + indicator + '\"/>\\n')\n",
    "        elif len(label) == 2:\n",
    "            if label[0] == 'smoker':\n",
    "                element = label[0].upper()\n",
    "                status = label[1]\n",
    "                file.write('\\t\\t<' + element + ' status=\"' + status + '\"/>\\n')\n",
    "            elif label[0] == 'family_hist':\n",
    "                element = label[0].upper()\n",
    "                indicator = label[1]\n",
    "                file.write('\\t\\t<' + element + ' indicator=\"' + indicator + '\"/>\\n')\n",
    "\n",
    "    if 'smoker.current' not in prediction and 'smoker.ever' not in prediction and 'smoker.never' not in prediction and 'smoker.past' not in prediction:\n",
    "        file.write('\\t\\t<SMOKER status=\"unknown\"/>\\n')\n",
    "    if 'family_hist.present' not in prediction:\n",
    "        file.write('\\t\\t<FAMILY_HIST indicator=\"not present\"/>\\n')\n",
    "\n",
    "    file.write(\"\\t</TAGS>\\n\")\n",
    "    file.write(\"</root>\\n\")\n",
    "    file.close()\n",
    "Ã—\n",
    "Drag and Drop\n",
    "The image will be downloaded by Fatkun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T18:59:25.549779Z",
     "start_time": "2019-06-19T18:59:25.542409Z"
    }
   },
   "outputs": [],
   "source": [
    "tagnames = ['CAD', 'DIABETES', 'FAMILY_HIST', 'HYPERLIPIDEMIA', 'HYPERTENSION', 'MEDICATION', 'OBESE', 'SMOKER']\n",
    "folder1 = '/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set1'\n",
    "folder2 = '/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set2'\n",
    "files1 = glob.glob(folder1+'/*.xml')\n",
    "files2 = glob.glob(folder2+'/*.xml')\n",
    "files = files1 + files2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T18:59:40.609856Z",
     "start_time": "2019-06-19T18:59:26.327212Z"
    }
   },
   "outputs": [],
   "source": [
    "data, data_list, labels, label_list = list(), list(), list(), list()\n",
    "\n",
    "for file in files:\n",
    "    root = minidom.parse(file)\n",
    "    annotation_objects = [root.getElementsByTagName(x) for x in tagnames]\n",
    "    annotations = [[[get_annotation(z, 'type1')\n",
    "                if z.tagName == 'MEDICATION' else get_annotation(z, 'status')\n",
    "                if z.tagName == 'SMOKER' else get_annotation(z, 'indicator')\n",
    "                for z in y.getElementsByTagName(y.tagName)] \n",
    "                for y in x] for x in annotation_objects]\n",
    "    annotations = [[y for y in x if len(y) > 0] for x in annotations if len(x) > 0]\n",
    "    annotations = list(set([y for x in [y for x in annotations for y in x] for y in x]))\n",
    "    annotations = [x for x in annotations if x[1] != 'family_hist.not_present' and x[1] != 'smoker.unknown']\n",
    "    annotations = [x for x in annotations if x[0] != '']\n",
    "    \n",
    "    annotations = combine_annotations(annotations)\n",
    "    annotations = [tokenise_annotation(x) for x in annotations]\n",
    "    annotations.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    text = root.getElementsByTagName(\"TEXT\")[0].firstChild.data\n",
    "    text = word_tokenize(text.lower())\n",
    "    \n",
    "    indices = [find_sublist(x[0], text) for x in annotations]\n",
    "    tags = ['O' for x in text]\n",
    "    annotate(tags, annotations, indices)\n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    text = [stemmer.stem(x) for x in text]\n",
    "    data.extend(text)\n",
    "    labels.extend(tags)\n",
    "    data_list.append(text)\n",
    "    label_list.append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T19:06:28.075324Z",
     "start_time": "2019-06-19T19:03:42.166733Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9c3cc22548a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9c3cc22548a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9c3cc22548a1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mencoded_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mencoded_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_indices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_indices = Counter(data).most_common()\n",
    "label_indices = Counter(labels).most_common()\n",
    "\n",
    "encoded_data = [[(i + 2) for y in x for i, a in enumerate(data_indices) if y == a[0]] for x in data_list]\n",
    "encoded_labels = [[(i + 1) for y in x for i, a in enumerate(label_indices) if y == a[0]] for x in label_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-19T14:36:18.942966Z",
     "start_time": "2019-06-19T14:36:18.830666Z"
    }
   },
   "outputs": [],
   "source": [
    "period_index = [i + 2 for i, x in enumerate(data_indices) if x[0] == \".\"][0]\n",
    "period_indices = [[i for i, y in enumerate(x) if y == period_index] for x in encoded_data]\n",
    "\n",
    "encoded_data = [isplit(x, (period_index,)) for x in encoded_data]\n",
    "replace_elements(encoded_labels, period_indices)\n",
    "encoded_labels = [isplit(x, (-1,)) for x in encoded_labels]\n",
    "\n",
    "print_data(encoded_data, encoded_labels, data_indices, label_indices)\n",
    "\n",
    "write_to_file('../data/dictionary.txt', data_indices, 2)\n",
    "write_to_file('../data/classes.txt', label_indices, 1)\n",
    "generate_files(encoded_data, encoded_labels, files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
