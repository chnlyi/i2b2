{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-08T15:26:36.463233Z",
     "start_time": "2019-05-08T15:23:37.397418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f198f5ce964332acaa532ae8dd08a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=521), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf75c1c0ebab48f2b023c839cbf865af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=269), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c4d3e2271a45e783e1ec982dc31d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=514), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration No: 1 started. Evaluating function at provided point.\n",
      "upsampling for 7 times...\n",
      "preparing features ...\n",
      "preparing embedding matrix ...\n",
      "preparing targets ...\n",
      "\n",
      "model summary:\n",
      "torch_tagger(\n",
      "  (word_embeddings): Embedding(44984, 30)\n",
      "  (lstm): LSTM(30, 128, bidirectional=True)\n",
      "  (hidden2tag): Linear(in_features=256, out_features=97, bias=True)\n",
      ")\n",
      "\n",
      "training model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b32bcade90b44109bcaee5dd9a248b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa1c25b5a6d4ad1a8bb54ba9e61987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6ff960ec2342e69667eae417a958c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=513), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.028239 \tValidation Loss: 0.009543\n",
      "Adiitional val metrics: - ROC-AUC: 0.985295 - Log-Loss: 0.667876 - Hamming-Loss: 0.001323 - Subset-Accuracy: 0.955057 - F1-Score: 0.937196\n",
      "Validation loss decreased (inf --> 0.009543).  Saving model ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0116fc92a1d4f9ebf037a386b9eef40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-efd2f9eab3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;31m# optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mres_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayes_opt_torch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m     \u001b[0mres_keras\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgp_minimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbayes_opt_keras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/optimizer/gp.py\u001b[0m in \u001b[0;36mgp_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, noise, n_jobs)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_restarts_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         callback=callback, n_jobs=n_jobs)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skopt/optimizer/base.py\u001b[0m in \u001b[0;36mbase_minimize\u001b[0;34m(func, dimensions, base_estimator, n_calls, n_random_starts, acq_func, acq_optimizer, x0, y0, random_state, verbose, callback, n_points, n_restarts_optimizer, xi, kappa, n_jobs)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# evaluate y0 if only x0 is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my0\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m         \u001b[0my0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0mn_calls\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;31m# record through tell function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-efd2f9eab3f3>\u001b[0m in \u001b[0;36mbayes_opt_specific\u001b[0;34m(space)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'torch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             f1 = no_pad_time_tuning(param, notes_train, labels_train, up_notes_train, up_labels_train, \n\u001b[0;32m--> 363\u001b[0;31m                                                  gold_labels_train, notes_test, labels_test, gold_labels_test, framework='torch')\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mf1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-efd2f9eab3f3>\u001b[0m in \u001b[0;36mno_pad_time_tuning\u001b[0;34m(param, notes_train, labels_train, up_notes_train, up_labels_train, gold_labels_train, notes_test, labels_test, gold_labels_test, framework, verbose)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0msentence_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-efd2f9eab3f3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNGRU, CuDNNLSTM, GRU, LSTM\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "from utils import process_data, multilabel_confusion_matrix, get_embedding_matrix, get_cat_labels, data_generator, get_all\n",
    "\n",
    "class torch_tagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, embedding_matrix=None):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        if embedding_matrix is None:\n",
    "            self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        else:\n",
    "            weight = torch.FloatTensor(embedding_matrix)\n",
    "            self.word_embeddings = nn.Embedding.from_pretrained(weight)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = torch.sigmoid(tag_space)\n",
    "        \n",
    "        return tag_scores\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "# Customized Evaluation for keras model\n",
    "class CustomEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = list(validation_data)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = []\n",
    "            for x in self.X_val:\n",
    "                y = np.squeeze(self.model.predict_on_batch(x))\n",
    "                y_pred.append(y)\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            y_pred_ham = y_pred > 0.5\n",
    "            y_val = np.concatenate(self.y_val)\n",
    "            roc = roc_auc_score(y_val, y_pred, average='micro')\n",
    "            loss = log_loss(y_val, y_pred)\n",
    "            ham = hamming_loss(y_val, y_pred_ham)\n",
    "            sub = accuracy_score(y_val, y_pred_ham)\n",
    "            f1 = f1_score(y_val, y_pred_ham, average='micro')\n",
    "            print(\"Adiitional val metrics: - ROC-AUC: %.6f - Log-Loss: %.6f - Hamming-Loss: %.6f - Subset-Accuracy: %.6f - F1-Score: %.6f\" % (roc, loss, ham, sub, f1))\n",
    "\n",
    "def no_pad_time_tuning(param, notes_train, labels_train, up_notes_train, up_labels_train, gold_labels_train, \n",
    "                       notes_test, labels_test, gold_labels_test, framework='keras', verbose=1):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    up = int(param['up'])\n",
    "    window_size = int(param['window_size'])\n",
    "    embed_size = int(param['embed_size'] * 10)\n",
    "    latent_dim = int(param['latent_dim'] * 64)\n",
    "    #dropout_rate = param['dropout_rate']\n",
    "    epochs = 30 #param['epochs']\n",
    "    max_features = 60000 #param['max_features']\n",
    "    category = False #param['category']\n",
    "    embedding = True #param['embedding']\n",
    "    model_type = 'CuDNNLSTM' #param['model_type']\n",
    "    \n",
    "    # upsampling\n",
    "    if up > 0:\n",
    "        if verbose != 0: print('upsampling for %d times...' % (up))\n",
    "        notes_train = [note + up * up_note for note, up_note in zip(notes_train, up_notes_train)]\n",
    "        labels_train = [label + up * up_label for label, up_label in zip(labels_train, up_labels_train)]\n",
    "#         if verbose != 0: print('upsampling done\\n')\n",
    "    notes = notes_train + notes_test\n",
    "    labels = labels_train + labels_test\n",
    "    gold_labels = gold_labels_train + gold_labels_test\n",
    "    \n",
    "    # prepare features\n",
    "    if verbose != 0: print('preparing features ...')\n",
    "    X_txt = [' '.join(i) for i in notes]\n",
    "    X_train_txt = [' '.join(i) for i in notes_train]\n",
    "    X_test_txt = [' '.join(i) for i in notes_test]\n",
    "    tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "    tokenizer.fit_on_texts(X_txt)\n",
    "    X_seq = tokenizer.texts_to_sequences(X_txt) \n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train_txt) \n",
    "    X_test_seq = tokenizer.texts_to_sequences(X_test_txt) \n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "#     if verbose != 0: print('preparing features done\\n')\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    if embedding:\n",
    "        if verbose != 0: print('preparing embedding matrix ...')\n",
    "        w2v = Word2Vec(notes, size=embed_size, window=window_size, min_count=1, workers=4)\n",
    "        embedding_index = dict(zip(w2v.wv.index2word, w2v.wv.vectors))\n",
    "        embedding_matrix = get_embedding_matrix(embedding_index=embedding_index, word_index=word_index, max_features=max_features, embed_size=embed_size)\n",
    "#         if verbose != 0: print('preparing embedding matrix done\\n')\n",
    "        \n",
    "    # prepare targets\n",
    "    if verbose != 0: print('preparing targets ...')\n",
    "    if category:\n",
    "        # prepare cagtegory label targets\n",
    "        labels = [[set([get_cat_labels(i) for i in list(j)]) for j in k] for k in labels]\n",
    "        labels_train = [[set([get_cat_labels(i) for i in list(j)]) for j in k] for k in labels_train]\n",
    "        labels_test = [[set([get_cat_labels(i) for i in list(j)]) for j in k] for k in labels_test]\n",
    "    all_labels = [label for notes_label in labels for label in notes_label]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    mlb.fit(all_labels)\n",
    "    num_labels = len(mlb.classes_)\n",
    "    Y_train = []\n",
    "    Y_test = []\n",
    "    for i in labels_train:\n",
    "        l = mlb.transform(i)\n",
    "        Y_train.append(l)\n",
    "    for i in labels_test:\n",
    "        l = mlb.transform(i)\n",
    "        Y_test.append(l)\n",
    "#     if verbose != 0: print('preparing targets done\\n')\n",
    "        \n",
    "    if framework == 'torch':\n",
    "    \n",
    "        # model summary\n",
    "        model = torch_tagger(embed_size, latent_dim, nb_words, num_labels, embedding_matrix).cuda()\n",
    "        loss_function = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "        if verbose != 0: print('\\nmodel summary:')\n",
    "        if verbose != 0: print(model)\n",
    "\n",
    "        # model training\n",
    "        if verbose != 0: print('\\ntraining model ...')\n",
    "        for epoch in tnrange(epochs):  \n",
    "            train_loss = 0.0\n",
    "            model.train()\n",
    "            for x, y in tqdm_notebook(zip(X_train_seq, Y_train), total=len(Y_train)):\n",
    "                optimizer.zero_grad()\n",
    "                sentence_in = torch.tensor(x).cuda()\n",
    "                targets = torch.FloatTensor(y).cuda()\n",
    "                tag_scores = model(sentence_in)\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_loss = train_loss/len(Y_train)\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            model.eval()\n",
    "            Y_pred = []\n",
    "            for i, (x, y) in tqdm_notebook(enumerate(zip(X_test_seq[:513], Y_test[:513])), total=len(Y_test[:513])):   \n",
    "                sentence_in = torch.tensor(x).cuda()\n",
    "                targets = torch.FloatTensor(y).cuda()\n",
    "                tag_scores = model(sentence_in)\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                valid_loss += loss.item()# * sentence_in.size(0)\n",
    "                Y_pred.append(tag_scores.detach().cpu().numpy())\n",
    "            valid_loss = valid_loss/len(Y_test[:513])\n",
    "            Y_pred_concat = np.concatenate(Y_pred)\n",
    "            Y_pred_ham = Y_pred_concat > 0.5\n",
    "            Y_val = np.concatenate(Y_test[:513])\n",
    "            roc = roc_auc_score(Y_val, Y_pred_concat, average='micro')\n",
    "            loss = log_loss(Y_val, Y_pred_concat)\n",
    "            ham = hamming_loss(Y_val, Y_pred_ham)\n",
    "            sub = accuracy_score(Y_val, Y_pred_ham)\n",
    "            f1 = f1_score(Y_val, Y_pred_ham, average='micro')\n",
    "            # print training/validation statistics \n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch + 1, train_loss, valid_loss))\n",
    "            print(\"Adiitional val metrics: - ROC-AUC: %.6f - Log-Loss: %.6f - Hamming-Loss: %.6f - Subset-Accuracy: %.6f - F1-Score: %.6f\" % (roc, loss, ham, sub, f1))\n",
    "\n",
    "            early_stopping(valid_loss, model)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "    \n",
    "    elif framework == 'keras':           \n",
    "            \n",
    "        # model function with pretrained embedding matrix and Timedistributed\n",
    "        def get_model(nb_words, num_labels, model_type, embedding):\n",
    "            inp = Input(shape=(None, ))\n",
    "            if embedding:\n",
    "                x = Embedding(nb_words, embed_size, weights=[embedding_matrix])(inp)\n",
    "            else:    \n",
    "                x = Embedding(nb_words, embed_size)(inp)\n",
    "            if model_type=='CuDNNGRU':\n",
    "                x = Bidirectional(CuDNNGRU(latent_dim, return_sequences=True))(x)\n",
    "            elif model_type=='GRU':\n",
    "                x = Bidirectional(GRU(latent_dim, return_sequences=True))(x)\n",
    "            elif model_type=='CuDNNLSTM':\n",
    "                x = Bidirectional(CuDNNLSTM(latent_dim, return_sequences=True))(x)\n",
    "            elif model_type=='LSTM':\n",
    "                x = Bidirectional(LSTM(latent_dim, return_sequences=True))(x)\n",
    "            else:\n",
    "                raise ValueError('Please specify model_type as one of the following:n\\CuDNNGRU, CuDNNLSTM, GRU, LSTM')\n",
    "            #x = SeqSelfAttention(attention_width=15, attention_activation='sigmoid')(x)\n",
    "            outp = TimeDistributed(Dense((num_labels), activation=\"sigmoid\"))(x)\n",
    "\n",
    "            model = Model(inputs=inp, outputs=outp)\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'])\n",
    "\n",
    "            return model\n",
    "\n",
    "        # model summary\n",
    "        model = get_model(nb_words=nb_words, num_labels=num_labels, model_type=model_type, embedding=embedding)\n",
    "        if verbose != 0: print('\\nmodel summary:')\n",
    "        if verbose != 0: print(model.summary())\n",
    "\n",
    "        # model training\n",
    "        if verbose != 0: print('\\ntraining model ...')\n",
    "        custevl = CustomEvaluation(validation_data=(X_test_seq, Y_test), interval=1)\n",
    "        earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')\n",
    "        train_gen = data_generator(X_train_seq, Y_train)\n",
    "        test_gen = data_generator(X_test_seq, Y_test)\n",
    "        v = 1 if verbose != 0 else 0  \n",
    "        hist = model.fit_generator(train_gen,\n",
    "                                    steps_per_epoch=len(Y_train),\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=test_gen,\n",
    "                                    validation_steps=len(Y_test),\n",
    "                                    callbacks=[custevl, earlystop],\n",
    "                                    verbose=v)\n",
    "#         if verbose != 0: print('training model done')\n",
    "\n",
    "        # prediction of test data\n",
    "#         if verbose != 0: print('predicting test data ...')\n",
    "        Y_pred = []\n",
    "        for x in X_test_seq[:513]:\n",
    "            x = np.array(x).reshape((1,-1))\n",
    "            y_pred = np.squeeze(model.predict_on_batch(x))\n",
    "            Y_pred.append(y_pred)\n",
    "        Y_pred_concat = np.concatenate(Y_pred)\n",
    "        Y_val = np.concatenate(Y_test)\n",
    "#         if verbose != 0: print('predicting test data done\\n')\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('wrong framework entered')\n",
    "        \n",
    "    # confusion matrix \n",
    "    if verbose == 2: \n",
    "        cm = multilabel_confusion_matrix(Y_val, np.where(Y_pred_concat > 0.5, 1, 0))\n",
    "        for i, j in zip(cm, mlb.classes_):\n",
    "            print(j+':\\n', i,'\\n')\n",
    "\n",
    "    # prepare gold label targets\n",
    "    if verbose != 0: print('predicting gold label targets ...')\n",
    "    gold_labels_pred = [{i for s in mlb.inverse_transform(y_pred>0.5) for i in s if i != 'O'} for y_pred in Y_pred]\n",
    "    gmlb = MultiLabelBinarizer()\n",
    "    gmlb.fit(gold_labels)\n",
    "    num_gold_labels = len(gmlb.classes_)\n",
    "    Y_gold_test = gmlb.transform(gold_labels_test[:513])\n",
    "    Y_gold_pred = gmlb.transform(gold_labels_pred)\n",
    "    if verbose != 0: print('predicting gold label targets done\\n')\n",
    "\n",
    "    # confusion matrix for gold label\n",
    "    if verbose == 2: \n",
    "        gcm = multilabel_confusion_matrix(np.concatenate(Y_gold_test), np.concatenate(Y_gold_pred))\n",
    "        for i, j in zip(gcm, gmlb.classes_):\n",
    "            print(j+':\\n', i,'\\n')\n",
    "\n",
    "    # f1 scores for gold label\n",
    "    f1 = f1_score(Y_gold_test, Y_gold_pred, average='micro')\n",
    "    print('Parameters: up = %d, window_size = %d, embed_size = %d, latent_dim = %d' % (up, window_size, embed_size, latent_dim))\n",
    "    print('\\nF1 Scores for global labels:\\nALL (average=\"micro\"):', f1)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    results_file = \"results_\"+framework+\".txt\"\n",
    "    \n",
    "    with open(results_file,\"a\") as f:\n",
    "        f.write('Parameters: up = %d, window_size = %d, embed_size = %d, latent_dim = %d' % (up, window_size, embed_size, latent_dim))\n",
    "        f.write('\\nF1 Scores for global labels(average=\"micro\"): %.3f; Running time: %.1f\\n' % (f1, elapsed_time))\n",
    "        \n",
    "    if verbose == 2: \n",
    "        f1_all = f1_score(Y_gold_test, Y_gold_pred, average=None)\n",
    "        for i, j in zip(f1_all, gmlb.classes_):\n",
    "            print(j+': '+str(i))\n",
    "    \n",
    "    print('\\n\\n')\n",
    "          \n",
    "    return f1\n",
    "\n",
    "def bayes_opt(space, framework='keras'):\n",
    "    \n",
    "    def bayes_opt_specific(space):\n",
    "        param = {\n",
    "                'up': space[0],               # Times of upsampling for training data\n",
    "                'window_size': space[1],                # Window size for word2vec\n",
    "                'embed_size': space[2],                # Length of the vector that we willl get from the embedding layer\n",
    "                'latent_dim': space[3]}               # Hidden layers dimension \n",
    "                #'dropout_rate': space[4],             # Rate of the dropout layers\n",
    "                #'epochs': space[0],                    # Number of epochs\n",
    "                #'max_features': space[0],           # Max num of vocabulary\n",
    "                #'category': space[0],               # Is categoty labels\n",
    "                #'embedding': space[0],               # Using pre-made embedidng matrix as weight\n",
    "                #'model_type': space[0]\n",
    "                #}\n",
    "            \n",
    "        if framework == 'keras':\n",
    "            f1 = no_pad_time_tuning(param, notes_train, labels_train, up_notes_train, up_labels_train, \n",
    "                                                 gold_labels_train, notes_test, labels_test, gold_labels_test)\n",
    "        elif framework == 'torch':\n",
    "            f1 = no_pad_time_tuning(param, notes_train, labels_train, up_notes_train, up_labels_train, \n",
    "                                                 gold_labels_train, notes_test, labels_test, gold_labels_test, framework='torch')\n",
    "\n",
    "        return (-f1)\n",
    "    \n",
    "    return bayes_opt_specific\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # loading data \n",
    "    if os.path.exists('loaded_data.dat'):\n",
    "        \n",
    "        with open('loaded_data.dat','rb') as f:\n",
    "            notes_train = pickle.load(f)\n",
    "            labels_train = pickle.load(f)\n",
    "            up_notes_train = pickle.load(f)\n",
    "            up_labels_train = pickle.load(f)\n",
    "            gold_labels_train = pickle.load(f)\n",
    "            notes_test = pickle.load(f)\n",
    "            labels_test = pickle.load(f)\n",
    "            gold_labels_test = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        notes_train_1, labels_train_1, up_notes_train_1, up_labels_train_1, gold_labels_train_1 = get_all('/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set1') \n",
    "        notes_train_2, labels_train_2, up_notes_train_2, up_labels_train_2, gold_labels_train_2 = get_all('/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set2') \n",
    "\n",
    "        notes_train = notes_train_1 + notes_train_2\n",
    "        labels_train = labels_train_1 + labels_train_2\n",
    "        up_notes_train = up_notes_train_1 + up_notes_train_2\n",
    "        up_labels_train = up_labels_train_1 + up_labels_train_2\n",
    "        gold_labels_train = gold_labels_train_1 + gold_labels_train_2\n",
    "\n",
    "        notes_test, labels_test, _1, _2, gold_labels_test = get_all('/host_home/data/i2b2/2014/testing/testing-RiskFactors-Complete')\n",
    "\n",
    "        with open('loaded_data.dat','wb') as f:\n",
    "            pickle.dump(notes_train, f)\n",
    "            pickle.dump(labels_train, f)\n",
    "            pickle.dump(up_notes_train, f)\n",
    "            pickle.dump(up_labels_train, f)\n",
    "            pickle.dump(gold_labels_train, f)\n",
    "            pickle.dump(notes_test, f)\n",
    "            pickle.dump(labels_test, f)\n",
    "            pickle.dump(gold_labels_test, f)\n",
    "\n",
    "            \n",
    "    # loading parameters space\n",
    "    space = [Integer(5, 10, name='up'),\n",
    "            Integer(3, 7, name='window_size'),\n",
    "            Integer(2, 4, name='embed_size'),\n",
    "            Integer(1, 3, name='latent_dim')]\n",
    "            #Real(0, 0.3, name='dropout_rate')\n",
    "            #Integer(30, 30, name='epochs'),\n",
    "            #Integer(1, 60000, name='max_features'),\n",
    "            #Categorical([False], name='category'),\n",
    "            #Categorical([True], name='embedding'),\n",
    "            #Categorical(['CuDNNLSTM'], name='model_type')]\n",
    "    \n",
    "    # initial parameters       \n",
    "    x0 = [7, 4, 3, 2]\n",
    "    \n",
    "    bayes_opt_torch = bayes_opt(space, framework='torch')\n",
    "    bayes_opt_keras = bayes_opt(space, framework='keras')\n",
    "    \n",
    "    # optimization\n",
    "    res_torch = gp_minimize(bayes_opt_torch, space, x0=x0, n_calls=50, verbose=True)\n",
    "    res_keras = gp_minimize(bayes_opt_keras, space, x0=x0, n_calls=50, verbose=True)\n",
    "    \n",
    "    with open(\"res_final.txt\",\"a\") as f:\n",
    "        f.write('opt by torch:\\n%s\\n' % str(res_torch))\n",
    "        f.write('opt by keras:\\n%s\\n' % str(res_keras))\n",
    "        \n",
    "    # python command: python heart_no_pad_opt_comparison.py > heart_no_pad_opt_comparison.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
