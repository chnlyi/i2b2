{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:17:37.314639Z",
     "start_time": "2019-07-08T15:17:36.354432Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import nltk\n",
    "import glob\n",
    "import itertools\n",
    "from xml.dom import minidom\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from collections import Counter\n",
    "\n",
    "# word_tokenize = TreebankWordTokenizer().tokenize\n",
    "\n",
    "def get_annotation(element, indicator):\n",
    "    if element.tagName == 'SMOKER' or element.tagName == 'FAMILY_HIST':\n",
    "        return (element.getAttribute('text').strip().lower(), element.tagName.lower() + '.' + \n",
    "                element.getAttribute(indicator).lower().strip().replace(' ', '_'))\n",
    "    else:\n",
    "        return (element.getAttribute('text').strip().lower(), element.tagName.lower() + '.' + \n",
    "                element.getAttribute(indicator).lower().strip().replace(' ', '_'), \n",
    "                element.getAttribute('time').lower().strip().replace(' ', '_'))\n",
    "    \n",
    "def tokenise_annotation(annotation):\n",
    "    return (word_tokenize(annotation[0]), annotation[1])\n",
    "\n",
    "def combine_annotations(annotations):\n",
    "    types = list()\n",
    "    results = list()\n",
    "    n = 0\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) == 3:\n",
    "            types.append((annotation[1], annotation[2]))\n",
    "    for annotation in annotations:\n",
    "        if len(annotation) == 3:\n",
    "            if ((annotation[1], 'before_dct') in types and \n",
    "                (annotation[1], 'during_dct') in types and \n",
    "                (annotation[1], 'after_dct') in types):\n",
    "                 results.append((annotation[0], annotation[1] + '.continuing'))\n",
    "            else:\n",
    "                results.append((annotation[0], annotation[1] + '.' + annotation[2]))\n",
    "        else:\n",
    "            results.append(annotation)\n",
    "    return list(set(results))\n",
    "\n",
    "def find_sublist(sublist, alist):\n",
    "    indices = list()\n",
    "    for index in (i for i, e in enumerate(alist) if e == sublist[0]):\n",
    "        if alist[index:index + len(sublist)] == sublist:\n",
    "            indices.append((index, index + len(sublist) - 1))\n",
    "    return indices\n",
    "\n",
    "def annotate(tags, annotations, indices):\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices[i])):\n",
    "            for k in range(indices[i][j][0], indices[i][j][1] + 1):\n",
    "                tags[k] = 'I-' + annotations[i][1]\n",
    "\n",
    "def isplit(iterable, splitters):\n",
    "    return [list(g) for k, g in itertools.groupby(iterable, lambda x: x in splitters) if not k]\n",
    "\n",
    "def replace_elements(alist, indices):\n",
    "    for i in range(len(indices)):\n",
    "        for j in range(len(indices[i])):\n",
    "            alist[i][indices[i][j]] = -1\n",
    "            \n",
    "def write_to_file(filename, data, index):\n",
    "    file = open(filename, 'w')\n",
    "    for i in range(len(data)):\n",
    "        file.write(\"%d %s\\n\" % (i + index, data[i][0]))\n",
    "    file.close()\n",
    "\n",
    "def generate_files(data, labels, files):\n",
    "    paths = ['../models/cnn/data/training/', '../models/rnn/data/training/', '../models/lstm/data/training/']\n",
    "    for i in range(0, len(data)):\n",
    "        for path in paths:\n",
    "            file = open(path + files[i][17:-4] + '.txt', 'w')\n",
    "            for j in range(0, len(data[i])):\n",
    "                file.write(','.join(str(x) for x in data[i][j]) + ' ' + (','.join(str(x) for x in labels[i][j])) + '\\n')\n",
    "            file.close()\n",
    "    \n",
    "def print_data(encoded_data, encoded_labels, data_indices, label_indices):\n",
    "    for i in range(len(encoded_data)):\n",
    "        for j in range(len(encoded_data[i])):\n",
    "            for k in range(len(encoded_data[i][j])):\n",
    "                print(data_indices[encoded_data[i][j][k] - 2][0] + \" \" + \n",
    "                    label_indices[encoded_labels[i][j][k] - 1][0])\n",
    "\n",
    "def get_chunks(text, max_length=256, ch='\\n'):\n",
    "    tok = TreebankWordTokenizer()\n",
    "    words_end = [i[1] for i in tok.span_tokenize(text)]\n",
    "    chunks = []\n",
    "    while len(words_end) > max_length:\n",
    "        all_ch = [i for i, ltr in enumerate(text) if ltr == ch]\n",
    "        if not all_ch: \n",
    "            print('uh oh')\n",
    "            break\n",
    "        x = 0\n",
    "        for i in all_ch:\n",
    "            if i < words_end[max_length]:\n",
    "                x = i\n",
    "        if x == 0: break\n",
    "        chunk = text[:x]\n",
    "        chunks.append(chunk)\n",
    "        text = text[x:]\n",
    "        words_end = [i[1] for i in tok.span_tokenize(text)]\n",
    "    chunks.append(text)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:17:37.325980Z",
     "start_time": "2019-07-08T15:17:37.318547Z"
    }
   },
   "outputs": [],
   "source": [
    "tagnames = ['CAD', 'DIABETES', 'FAMILY_HIST', 'HYPERLIPIDEMIA', 'HYPERTENSION', 'MEDICATION', 'OBESE', 'SMOKER']\n",
    "folder1 = '/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set1'\n",
    "folder2 = '/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set2'\n",
    "files1 = glob.glob(folder1+'/*.xml')\n",
    "files2 = glob.glob(folder2+'/*.xml')\n",
    "files = files1 + files2\n",
    "\n",
    "max_length=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:20:44.071221Z",
     "start_time": "2019-07-08T15:20:28.477966Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a50c0f457ee477f89028c9a412b04c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=790), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data, data_list, labels, label_list = list(), list(), list(), list()\n",
    "tagged_sents = list()\n",
    "\n",
    "for file in tqdm(files):\n",
    "    root = minidom.parse(file)\n",
    "    annotation_objects = [root.getElementsByTagName(x) for x in tagnames]\n",
    "    annotations = [[[get_annotation(z, 'type1')\n",
    "                if z.tagName == 'MEDICATION' else get_annotation(z, 'status')\n",
    "                if z.tagName == 'SMOKER' else get_annotation(z, 'indicator')\n",
    "                for z in y.getElementsByTagName(y.tagName)] \n",
    "                for y in x] for x in annotation_objects]\n",
    "    annotations = [[y for y in x if len(y) > 0] for x in annotations if len(x) > 0]\n",
    "    annotations = list(set([y for x in [y for x in annotations for y in x] for y in x]))\n",
    "    annotations = [x for x in annotations if x[1] != 'family_hist.not_present' and x[1] != 'smoker.unknown']\n",
    "    annotations = [x for x in annotations if x[0] != '']\n",
    "    \n",
    "    annotations = combine_annotations(annotations)\n",
    "    annotations = [tokenise_annotation(x) for x in annotations]\n",
    "    annotations.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    text = root.getElementsByTagName(\"TEXT\")[0].firstChild.data\n",
    "    text = word_tokenize(text.lower())\n",
    "#     text_chunks = get_chunks(text, max_length=max_length)\n",
    "\n",
    "    indices = [find_sublist(x[0], text) for x in annotations]\n",
    "    tags = ['O' for x in text]\n",
    "    annotate(tags, annotations, indices) \n",
    "    \n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    text = [stemmer.stem(x) for x in text]\n",
    "    \n",
    "    tagged_sent = list(zip(text, tags)) \n",
    "\n",
    "#     for i in text_chunks:\n",
    "#         t_chunk = word_tokenize(i.lower())\n",
    "#         ind_chunk = [find_sublist(x[0], i) for x in annotations]\n",
    "#         tg_chunk = ['O' for x in i]\n",
    "#         annotate(tg_chunk, annotations, ind_chunk)\n",
    "#         t_chunk = [stemmer.stem(x) for x in t_chunk]\n",
    "#         tagged_sent = list(zip(t_chunk, tg_chunk)) \n",
    "#         tagged_sents.append(tagged_sent)\n",
    "        \n",
    "    if len(tagged_sent) > max_length:\n",
    "        ls_tagged_sent = [tagged_sent[i * max_length:(i + 1) * max_length] for i in range((len(tagged_sent) + max_length - 1) // max_length )]\n",
    "        tagged_sents.extend(ls_tagged_sent)\n",
    "    else:\n",
    "        tagged_sents.append(tagged_sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:20:49.699691Z",
     "start_time": "2019-07-08T15:20:49.685238Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('record', 'O'),\n",
       " ('date', 'O'),\n",
       " (':', 'O'),\n",
       " ('2067-05-03', 'O'),\n",
       " ('narrat', 'O'),\n",
       " ('histori', 'O'),\n",
       " ('55', 'O'),\n",
       " ('yo', 'O'),\n",
       " ('woman', 'O'),\n",
       " ('who', 'O'),\n",
       " ('present', 'O'),\n",
       " ('for', 'O'),\n",
       " ('f/u', 'O'),\n",
       " ('seen', 'O'),\n",
       " ('in', 'O'),\n",
       " ('cardiac', 'O'),\n",
       " ('rehab', 'O'),\n",
       " ('local', 'O'),\n",
       " ('last', 'O'),\n",
       " ('week', 'O'),\n",
       " ('and', 'O'),\n",
       " ('bp', 'I-hypertension.high_bp.before_dct'),\n",
       " ('170/80', 'I-hypertension.high_bp.before_dct'),\n",
       " ('.', 'I-hypertension.high_bp.before_dct'),\n",
       " ('they', 'O'),\n",
       " ('call', 'O'),\n",
       " ('us', 'O'),\n",
       " ('and', 'O'),\n",
       " ('we', 'O'),\n",
       " ('increas', 'O'),\n",
       " ('her', 'O'),\n",
       " ('hctz', 'I-medication.diuretic.continuing'),\n",
       " ('to', 'O'),\n",
       " ('25', 'O'),\n",
       " ('mg', 'O'),\n",
       " ('from', 'O'),\n",
       " ('12.5', 'O'),\n",
       " ('mg.', 'O'),\n",
       " ('state', 'O'),\n",
       " ('her', 'O'),\n",
       " ('bp', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('were', 'O'),\n",
       " ('fine', 'O'),\n",
       " ('there', 'O'),\n",
       " ('sinc', 'O'),\n",
       " ('-', 'O'),\n",
       " ('130-140/70-80', 'O'),\n",
       " ('.', 'O'),\n",
       " ('saw', 'O'),\n",
       " ('dr', 'O'),\n",
       " ('oakley', 'O'),\n",
       " ('4/5/67', 'O'),\n",
       " ('-', 'O'),\n",
       " ('she', 'O'),\n",
       " ('was', 'O'),\n",
       " ('happi', 'O'),\n",
       " ('with', 'O'),\n",
       " ('result', 'O'),\n",
       " ('of', 'O'),\n",
       " ('ett', 'O'),\n",
       " ('at', 'O'),\n",
       " ('clarkfield', 'O'),\n",
       " ('.', 'O'),\n",
       " ('to', 'O'),\n",
       " ('f/u', 'O'),\n",
       " ('7/67', 'O'),\n",
       " ('.', 'O'),\n",
       " ('no', 'O'),\n",
       " ('cp', 'O'),\n",
       " (\"'s\", 'O'),\n",
       " ('sinc', 'O'),\n",
       " ('last', 'O'),\n",
       " ('admit', 'O'),\n",
       " ('.', 'O'),\n",
       " ('back', 'O'),\n",
       " ('to', 'O'),\n",
       " ('work', 'O'),\n",
       " ('and', 'O'),\n",
       " ('start', 'O'),\n",
       " ('to', 'O'),\n",
       " ('walk', 'O'),\n",
       " ('.', 'O'),\n",
       " ('no', 'O'),\n",
       " ('wt', 'O'),\n",
       " ('loss', 'O'),\n",
       " ('and', 'O'),\n",
       " ('discourag', 'O'),\n",
       " ('by', 'O'),\n",
       " ('this', 'O'),\n",
       " (',', 'O'),\n",
       " ('but', 'O'),\n",
       " ('just', 'O'),\n",
       " ('start', 'O'),\n",
       " ('to', 'O'),\n",
       " ('exercis', 'O'),\n",
       " ('.', 'O'),\n",
       " ('no', 'I-smoker.past'),\n",
       " ('smoke', 'I-smoker.past'),\n",
       " ('for', 'I-smoker.past'),\n",
       " ('3', 'I-smoker.past'),\n",
       " ('month', 'I-smoker.past'),\n",
       " ('now', 'I-smoker.past'),\n",
       " ('!', 'I-smoker.past'),\n",
       " ('still', 'O'),\n",
       " ('with', 'O'),\n",
       " ('hotflash', 'O'),\n",
       " (',', 'O'),\n",
       " ('wake', 'O'),\n",
       " ('her', 'O'),\n",
       " ('up', 'O'),\n",
       " ('at', 'O'),\n",
       " ('night', 'O'),\n",
       " ('.', 'O'),\n",
       " ('problem', 'O'),\n",
       " ('fh', 'O'),\n",
       " ('breast', 'O'),\n",
       " ('cancer', 'O'),\n",
       " ('37', 'O'),\n",
       " ('yo', 'O'),\n",
       " ('s', 'O'),\n",
       " ('fh', 'O'),\n",
       " ('myocardi', 'O'),\n",
       " ('infarct', 'O'),\n",
       " ('mother', 'O'),\n",
       " ('die', 'O'),\n",
       " ('66', 'O'),\n",
       " ('yo', 'O')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:21:02.096194Z",
     "start_time": "2019-07-08T15:21:01.130788Z"
    }
   },
   "outputs": [],
   "source": [
    "# tagged_sents = tagged_sents[:4]\n",
    "\n",
    "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))\n",
    "\n",
    "# By convention, the 0'th slot is reserved for padding.\n",
    "tags = [\"<pad>\"] + tags\n",
    "\n",
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}\n",
    "\n",
    "# Let's split the data into train and test (or eval)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
    "len(train_data), len(test_data)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "\n",
    "class PosDataset(data.Dataset):\n",
    "    def __init__(self, tagged_sents):\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for sent in tagged_sents:\n",
    "            words = [word_pos[0] for word_pos in sent]\n",
    "            tags = [word_pos[1] for word_pos in sent]\n",
    "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "\n",
    "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:21:02.972007Z",
     "start_time": "2019-07-08T15:21:02.958084Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:21:08.584446Z",
     "start_time": "2019-07-08T15:21:08.571388Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        \n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:21:09.155372Z",
     "start_time": "2019-07-08T15:21:09.131916Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(\"step: {}, loss: {}\".format(i, loss.item()))\n",
    "\n",
    "def eval(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"result\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
    "            fout.write(\"\\n\")\n",
    "            \n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
    "\n",
    "    print(\"acc=%.2f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:21:17.869522Z",
     "start_time": "2019-07-08T15:21:09.774111Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "train_dataset = PosDataset(train_data)\n",
    "eval_dataset = PosDataset(test_data)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=4,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T15:26:27.667841Z",
     "start_time": "2019-07-08T15:21:17.872763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 4.719751358032227\n",
      "step: 10, loss: 0.31458622217178345\n",
      "step: 20, loss: 0.357718825340271\n",
      "step: 30, loss: 0.5714498162269592\n",
      "step: 40, loss: 1.133885383605957\n",
      "step: 50, loss: 0.35433152318000793\n",
      "step: 60, loss: 0.2622257471084595\n",
      "step: 70, loss: 0.2912757396697998\n",
      "step: 80, loss: 0.2950582206249237\n",
      "step: 90, loss: 0.30344244837760925\n",
      "step: 100, loss: 0.24819862842559814\n",
      "step: 110, loss: 0.11084044724702835\n",
      "step: 120, loss: 0.27604228258132935\n",
      "step: 130, loss: 0.30938076972961426\n",
      "step: 140, loss: 0.2378695160150528\n",
      "step: 150, loss: 0.22441233694553375\n",
      "step: 160, loss: 0.25023359060287476\n",
      "step: 170, loss: 0.12621469795703888\n",
      "step: 180, loss: 0.26662611961364746\n",
      "step: 190, loss: 0.20178253948688507\n",
      "step: 200, loss: 0.08162254095077515\n",
      "step: 210, loss: 0.19173061847686768\n",
      "step: 220, loss: 0.23193714022636414\n",
      "step: 230, loss: 0.1111311987042427\n",
      "step: 240, loss: 0.3526153266429901\n",
      "step: 250, loss: 0.17804056406021118\n",
      "step: 260, loss: 0.31672778725624084\n",
      "step: 270, loss: 0.060564734041690826\n",
      "step: 280, loss: 0.2091185450553894\n",
      "step: 290, loss: 0.7153937816619873\n",
      "step: 300, loss: 0.2438393086194992\n",
      "step: 310, loss: 0.3791603744029999\n",
      "step: 320, loss: 0.1389472484588623\n",
      "step: 330, loss: 0.038154296576976776\n",
      "step: 340, loss: 0.04879344627261162\n",
      "step: 350, loss: 0.1347741186618805\n",
      "step: 360, loss: 0.6064079403877258\n",
      "step: 370, loss: 0.19019633531570435\n",
      "step: 380, loss: 0.0949302390217781\n",
      "step: 390, loss: 0.21376384794712067\n",
      "step: 400, loss: 0.2815483808517456\n",
      "step: 410, loss: 0.1927875429391861\n",
      "step: 420, loss: 0.11442433297634125\n",
      "step: 430, loss: 0.08519064635038376\n",
      "step: 440, loss: 0.1775866001844406\n",
      "step: 450, loss: 0.2151648849248886\n",
      "step: 460, loss: 0.08950024843215942\n",
      "step: 470, loss: 0.10540914535522461\n",
      "step: 480, loss: 0.23051203787326813\n",
      "step: 490, loss: 0.17492946982383728\n",
      "step: 500, loss: 0.10588333755731583\n",
      "step: 510, loss: 0.0633787214756012\n",
      "step: 520, loss: 0.4867529273033142\n",
      "step: 530, loss: 0.20053498446941376\n",
      "step: 540, loss: 0.010912608355283737\n",
      "step: 550, loss: 0.0765465497970581\n",
      "step: 560, loss: 0.08590273559093475\n",
      "step: 570, loss: 0.522050142288208\n",
      "step: 580, loss: 0.017401034012436867\n",
      "step: 590, loss: 0.181660994887352\n",
      "step: 600, loss: 0.13316574692726135\n",
      "step: 610, loss: 0.07342791557312012\n",
      "step: 620, loss: 0.1262718141078949\n",
      "step: 630, loss: 0.3224570155143738\n",
      "step: 640, loss: 0.11419350653886795\n",
      "step: 650, loss: 0.04602751135826111\n",
      "step: 660, loss: 0.07266034185886383\n",
      "step: 670, loss: 0.20157919824123383\n",
      "step: 680, loss: 0.7235985994338989\n",
      "step: 690, loss: 0.29397857189178467\n",
      "step: 700, loss: 0.13174766302108765\n",
      "step: 710, loss: 0.07305805385112762\n",
      "step: 720, loss: 0.051579758524894714\n",
      "step: 730, loss: 0.1906615048646927\n",
      "step: 740, loss: 0.12552718818187714\n",
      "step: 750, loss: 0.09966759383678436\n",
      "step: 760, loss: 0.3626124858856201\n",
      "step: 770, loss: 0.6477394700050354\n",
      "step: 780, loss: 0.30126914381980896\n",
      "step: 790, loss: 0.9166502356529236\n",
      "step: 800, loss: 0.07868163287639618\n",
      "step: 810, loss: 0.08419004827737808\n",
      "step: 820, loss: 0.13042640686035156\n",
      "step: 830, loss: 0.09421709179878235\n",
      "step: 840, loss: 0.01290123350918293\n",
      "step: 850, loss: 0.3360432982444763\n",
      "step: 860, loss: 0.06534039229154587\n",
      "step: 870, loss: 0.1917024850845337\n",
      "step: 880, loss: 0.054890234023332596\n",
      "step: 890, loss: 0.059598639607429504\n",
      "step: 900, loss: 0.3944152593612671\n",
      "step: 910, loss: 0.03990374878048897\n",
      "step: 920, loss: 0.20256046950817108\n",
      "step: 930, loss: 0.1684587597846985\n",
      "step: 940, loss: 0.18655449151992798\n",
      "step: 950, loss: 0.04331212490797043\n",
      "step: 960, loss: 0.0255964957177639\n",
      "step: 970, loss: 0.10726214200258255\n",
      "step: 980, loss: 0.05368910729885101\n",
      "step: 990, loss: 0.1276283860206604\n",
      "step: 1000, loss: 0.2203056961297989\n",
      "step: 1010, loss: 0.5344539284706116\n",
      "step: 1020, loss: 0.12676844000816345\n",
      "step: 1030, loss: 0.13593649864196777\n",
      "step: 1040, loss: 0.0589432567358017\n",
      "step: 1050, loss: 0.08084899932146072\n",
      "step: 1060, loss: 0.38189274072647095\n",
      "step: 1070, loss: 0.16287986934185028\n",
      "step: 1080, loss: 0.02668779529631138\n",
      "step: 1090, loss: 0.18660715222358704\n",
      "step: 1100, loss: 0.014239628799259663\n",
      "step: 1110, loss: 0.10637357085943222\n",
      "step: 1120, loss: 0.022712547332048416\n",
      "acc=0.96\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, optimizer, criterion)\n",
    "eval(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
