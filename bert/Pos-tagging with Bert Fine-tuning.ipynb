{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:18:36.744320Z",
     "start_time": "2019-07-05T14:18:36.736134Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import torch.optim as optim\n",
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:18:40.160642Z",
     "start_time": "2019-07-05T14:18:39.067050Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3914"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents = nltk.corpus.treebank.tagged_sents()\n",
    "len(tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:18:53.017353Z",
     "start_time": "2019-07-05T14:18:53.005560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Pierre', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('61', 'CD'),\n",
       "  ('years', 'NNS'),\n",
       "  ('old', 'JJ'),\n",
       "  (',', ','),\n",
       "  ('will', 'MD'),\n",
       "  ('join', 'VB'),\n",
       "  ('the', 'DT'),\n",
       "  ('board', 'NN'),\n",
       "  ('as', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('nonexecutive', 'JJ'),\n",
       "  ('director', 'NN'),\n",
       "  ('Nov.', 'NNP'),\n",
       "  ('29', 'CD'),\n",
       "  ('.', '.')],\n",
       " [('Mr.', 'NNP'),\n",
       "  ('Vinken', 'NNP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('chairman', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Elsevier', 'NNP'),\n",
       "  ('N.V.', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('the', 'DT'),\n",
       "  ('Dutch', 'NNP'),\n",
       "  ('publishing', 'VBG'),\n",
       "  ('group', 'NN'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sents[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:01:56.312843Z",
     "start_time": "2019-07-05T14:01:55.199994Z"
    }
   },
   "outputs": [],
   "source": [
    "tags = list(set(word_pos[1] for sent in tagged_sents for word_pos in sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:02:02.348093Z",
     "start_time": "2019-07-05T14:02:02.340234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-LRB-,-NONE-,SYM,,,NN,FW,VBD,WDT,``,'',WP$,RB,WRB,VBZ,RP,CD,VBP,POS,EX,NNPS,RBR,VBN,NNP,CC,PDT,#,TO,JJ,$,PRP,NNS,IN,VB,WP,JJR,:,-RRB-,PRP$,UH,DT,JJS,RBS,MD,.,VBG,LS\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:02:30.235293Z",
     "start_time": "2019-07-05T14:02:30.232221Z"
    }
   },
   "outputs": [],
   "source": [
    "# By convention, the 0'th slot is reserved for padding.\n",
    "tags = [\"<pad>\"] + tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:02:41.180670Z",
     "start_time": "2019-07-05T14:02:41.173586Z"
    }
   },
   "outputs": [],
   "source": [
    "tag2idx = {tag:idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx:tag for idx, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:02:49.662656Z",
     "start_time": "2019-07-05T14:02:48.285222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3522, 392)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's split the data into train and test (or eval)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(tagged_sents, test_size=.1)\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:02:57.741993Z",
     "start_time": "2019-07-05T14:02:57.053118Z"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:03:26.355563Z",
     "start_time": "2019-07-05T14:03:26.347899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:03:35.922288Z",
     "start_time": "2019-07-05T14:03:35.360112Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 213450/213450 [00:00<00:00, 1023670.05B/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:03:50.269241Z",
     "start_time": "2019-07-05T14:03:50.252781Z"
    }
   },
   "outputs": [],
   "source": [
    "class PosDataset(data.Dataset):\n",
    "    def __init__(self, tagged_sents):\n",
    "        sents, tags_li = [], [] # list of lists\n",
    "        for sent in tagged_sents:\n",
    "            words = [word_pos[0] for word_pos in sent]\n",
    "            tags = [word_pos[1] for word_pos in sent]\n",
    "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
    "            tags_li.append([\"<pad>\"] + tags + [\"<pad>\"])\n",
    "        self.sents, self.tags_li = sents, tags_li\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sents)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
    "\n",
    "        # We give credits only to the first piece.\n",
    "        x, y = [], [] # list of ids\n",
    "        is_heads = [] # list. 1: the token is the first piece of a word\n",
    "        for w, t in zip(words, tags):\n",
    "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
    "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            is_head = [1] + [0]*(len(tokens) - 1)\n",
    "\n",
    "            t = [t] + [\"<pad>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
    "            yy = [tag2idx[each] for each in t]  # (T,)\n",
    "\n",
    "            x.extend(xx)\n",
    "            is_heads.extend(is_head)\n",
    "            y.extend(yy)\n",
    "\n",
    "        assert len(x)==len(y)==len(is_heads), \"len(x)={}, len(y)={}, len(is_heads)={}\".format(len(x), len(y), len(is_heads))\n",
    "\n",
    "        # seqlen\n",
    "        seqlen = len(y)\n",
    "\n",
    "        # to string\n",
    "        words = \" \".join(words)\n",
    "        tags = \" \".join(tags)\n",
    "        return words, x, is_heads, tags, y, seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:03:57.764429Z",
     "start_time": "2019-07-05T14:03:57.756183Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad(batch):\n",
    "    '''Pads to the longest sample'''\n",
    "    f = lambda x: [sample[x] for sample in batch]\n",
    "    words = f(0)\n",
    "    is_heads = f(2)\n",
    "    tags = f(3)\n",
    "    seqlens = f(-1)\n",
    "    maxlen = np.array(seqlens).max()\n",
    "\n",
    "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
    "    x = f(1, maxlen)\n",
    "    y = f(-2, maxlen)\n",
    "\n",
    "\n",
    "    f = torch.LongTensor\n",
    "\n",
    "    return words, f(x), is_heads, tags, f(y), seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:13:47.438452Z",
     "start_time": "2019-07-05T14:13:47.430996Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:14:01.348207Z",
     "start_time": "2019-07-05T14:14:01.333054Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "        self.fc = nn.Linear(768, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: (N, T). int64\n",
    "        y: (N, T). int64\n",
    "        '''\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        if self.training:\n",
    "            self.bert.train()\n",
    "            encoded_layers, _ = self.bert(x)\n",
    "            enc = encoded_layers[-1]\n",
    "        else:\n",
    "            self.bert.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded_layers, _ = self.bert(x)\n",
    "                enc = encoded_layers[-1]\n",
    "        \n",
    "        logits = self.fc(enc)\n",
    "        y_hat = logits.argmax(-1)\n",
    "        return logits, y, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:14:09.954265Z",
     "start_time": "2019-07-05T14:14:09.941273Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(iterator):\n",
    "        words, x, is_heads, tags, y, seqlens = batch\n",
    "        _y = y # for monitoring\n",
    "        optimizer.zero_grad()\n",
    "        logits, y, _ = model(x, y) # logits: (N, T, VOCAB), y: (N, T)\n",
    "\n",
    "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
    "        y = y.view(-1)  # (N*T,)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%10==0: # monitoring\n",
    "            print(\"step: {}, loss: {}\".format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:14:20.276313Z",
     "start_time": "2019-07-05T14:14:20.256930Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, iterator):\n",
    "    model.eval()\n",
    "\n",
    "    Words, Is_heads, Tags, Y, Y_hat = [], [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            words, x, is_heads, tags, y, seqlens = batch\n",
    "\n",
    "            _, _, y_hat = model(x, y)  # y_hat: (N, T)\n",
    "\n",
    "            Words.extend(words)\n",
    "            Is_heads.extend(is_heads)\n",
    "            Tags.extend(tags)\n",
    "            Y.extend(y.numpy().tolist())\n",
    "            Y_hat.extend(y_hat.cpu().numpy().tolist())\n",
    "\n",
    "    ## gets results and save\n",
    "    with open(\"result\", 'w') as fout:\n",
    "        for words, is_heads, tags, y_hat in zip(Words, Is_heads, Tags, Y_hat):\n",
    "            y_hat = [hat for head, hat in zip(is_heads, y_hat) if head == 1]\n",
    "            preds = [idx2tag[hat] for hat in y_hat]\n",
    "            assert len(preds)==len(words.split())==len(tags.split())\n",
    "            for w, t, p in zip(words.split()[1:-1], tags.split()[1:-1], preds[1:-1]):\n",
    "                fout.write(\"{} {} {}\\n\".format(w, t, p))\n",
    "            fout.write(\"\\n\")\n",
    "            \n",
    "    ## calc metric\n",
    "    y_true =  np.array([tag2idx[line.split()[1]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "    y_pred =  np.array([tag2idx[line.split()[2]] for line in open('result', 'r').read().splitlines() if len(line) > 0])\n",
    "\n",
    "    acc = (y_true==y_pred).astype(np.int32).sum() / len(y_true)\n",
    "\n",
    "    print(\"acc=%.2f\"%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:14:57.682361Z",
     "start_time": "2019-07-05T14:14:29.144587Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 404400730/404400730 [00:19<00:00, 20434272.91B/s]\n"
     ]
    }
   ],
   "source": [
    "model = Net(vocab_size=len(tag2idx))\n",
    "model.to(device)\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-05T14:14:57.843294Z",
     "start_time": "2019-07-05T14:14:57.685881Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = PosDataset(train_data)\n",
    "eval_dataset = PosDataset(test_data)\n",
    "\n",
    "train_iter = data.DataLoader(dataset=train_dataset,\n",
    "                             batch_size=4,\n",
    "                             shuffle=True,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "test_iter = data.DataLoader(dataset=eval_dataset,\n",
    "                             batch_size=8,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1,\n",
    "                             collate_fn=pad)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-05T14:15:01.784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.8434646129608154\n",
      "step: 10, loss: 2.192378520965576\n",
      "step: 20, loss: 0.921332597732544\n",
      "step: 30, loss: 0.6827813386917114\n",
      "step: 40, loss: 0.3998739421367645\n",
      "step: 50, loss: 0.13900046050548553\n",
      "step: 60, loss: 0.11692921817302704\n",
      "step: 70, loss: 0.1865527331829071\n",
      "step: 80, loss: 0.2131170779466629\n",
      "step: 90, loss: 0.15794637799263\n",
      "step: 100, loss: 0.19296452403068542\n",
      "step: 110, loss: 0.14558951556682587\n",
      "step: 120, loss: 0.17431259155273438\n",
      "step: 130, loss: 0.12446510791778564\n",
      "step: 140, loss: 0.16832268238067627\n",
      "step: 150, loss: 0.30632010102272034\n",
      "step: 160, loss: 0.29049867391586304\n",
      "step: 170, loss: 0.07585407048463821\n",
      "step: 180, loss: 0.13735361397266388\n",
      "step: 190, loss: 0.1664280742406845\n",
      "step: 200, loss: 0.14477935433387756\n",
      "step: 210, loss: 0.18329165875911713\n",
      "step: 220, loss: 0.2329513430595398\n",
      "step: 230, loss: 0.11744561046361923\n",
      "step: 240, loss: 0.06952869892120361\n",
      "step: 250, loss: 0.10857828706502914\n",
      "step: 260, loss: 0.047143347561359406\n",
      "step: 270, loss: 0.10104138404130936\n",
      "step: 280, loss: 0.23856225609779358\n",
      "step: 290, loss: 0.1430252343416214\n",
      "step: 300, loss: 0.16816270351409912\n",
      "step: 310, loss: 0.12315952032804489\n",
      "step: 320, loss: 0.021834861487150192\n",
      "step: 330, loss: 0.18304358422756195\n",
      "step: 340, loss: 0.22017566859722137\n",
      "step: 350, loss: 0.11861450225114822\n",
      "step: 360, loss: 0.05568963661789894\n",
      "step: 370, loss: 0.04609527066349983\n",
      "step: 380, loss: 0.05291631817817688\n",
      "step: 390, loss: 0.14789555966854095\n",
      "step: 400, loss: 0.049443569034338\n",
      "step: 410, loss: 0.11086317896842957\n",
      "step: 420, loss: 0.02411719225347042\n",
      "step: 430, loss: 0.10971040278673172\n",
      "step: 440, loss: 0.10445268452167511\n",
      "step: 450, loss: 0.04521731659770012\n",
      "step: 460, loss: 0.18858279287815094\n",
      "step: 470, loss: 0.13147994875907898\n",
      "step: 480, loss: 0.12045839428901672\n",
      "step: 490, loss: 0.04669860377907753\n",
      "step: 500, loss: 0.05968504771590233\n",
      "step: 510, loss: 0.05002199858427048\n",
      "step: 520, loss: 0.06512682139873505\n",
      "step: 530, loss: 0.13207504153251648\n",
      "step: 540, loss: 0.15779617428779602\n",
      "step: 550, loss: 0.05856334790587425\n",
      "step: 560, loss: 0.14881479740142822\n",
      "step: 570, loss: 0.08184079080820084\n",
      "step: 580, loss: 0.2132478952407837\n",
      "step: 590, loss: 0.18287596106529236\n",
      "step: 600, loss: 0.09671090543270111\n",
      "step: 610, loss: 0.11525870114564896\n"
     ]
    }
   ],
   "source": [
    "train(model, train_iter, optimizer, criterion)\n",
    "eval(model, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
