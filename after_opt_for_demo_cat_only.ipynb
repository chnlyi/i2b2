{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-04T14:49:52.335532Z",
     "start_time": "2019-06-04T14:41:32.435957Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "Parameters (note: embed_size*10, latent_dim*64):\n",
      " {'up': 5, 'window_size': 3, 'embed_size': 5, 'latent_dim': 5, 'dropout_rate': 0.0, 'epochs': 20, 'category': 'cat_only'}\n",
      "********************************************************************************\n",
      "upsampling for 5 times...\n",
      "preparing features ...\n",
      "preparing pretrained embedding matrix ...\n",
      "preparing targets ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "model summary:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, None, 50)          2249200   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 640)         952320    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 640)         0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 300)         192300    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 9)           2709      \n",
      "=================================================================\n",
      "Total params: 3,396,529\n",
      "Trainable params: 3,396,529\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "training model ...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "790/790 [==============================] - 115s 145ms/step - loss: 0.0705 - acc: 0.9800 - val_loss: 0.0327 - val_acc: 0.9915\n",
      "Adiitional val metrics: - ROC-AUC: 0.983986 - Log-Loss: 0.869993 - Hamming-Loss: 0.012409 - Subset-Accuracy: 0.930654 - F1-Score: 0.944040\n",
      "Epoch 2/20\n",
      "790/790 [==============================] - 112s 141ms/step - loss: 0.0401 - acc: 0.9872 - val_loss: 0.0260 - val_acc: 0.9926\n",
      "Adiitional val metrics: - ROC-AUC: 0.988803 - Log-Loss: 0.570047 - Hamming-Loss: 0.010380 - Subset-Accuracy: 0.945764 - F1-Score: 0.952990\n",
      "Epoch 3/20\n",
      "790/790 [==============================] - 113s 143ms/step - loss: 0.0293 - acc: 0.9900 - val_loss: 0.0263 - val_acc: 0.9931\n",
      "Adiitional val metrics: - ROC-AUC: 0.990606 - Log-Loss: 0.448219 - Hamming-Loss: 0.010348 - Subset-Accuracy: 0.945700 - F1-Score: 0.953110\n",
      "Epoch 4/20\n",
      "790/790 [==============================] - 113s 143ms/step - loss: 0.0228 - acc: 0.9920 - val_loss: 0.0273 - val_acc: 0.9931\n",
      "Adiitional val metrics: - ROC-AUC: 0.991254 - Log-Loss: 0.395098 - Hamming-Loss: 0.011110 - Subset-Accuracy: 0.943348 - F1-Score: 0.949681\n",
      "predicting test data ...\n",
      "CAD:\n",
      " [[310396   1001]\n",
      " [  2523   1515]] \n",
      "\n",
      "DIABETES:\n",
      " [[313213    415]\n",
      " [   732   1075]] \n",
      "\n",
      "FAMILY_HIST:\n",
      " [[315221      0]\n",
      " [   214      0]] \n",
      "\n",
      "HYPERLIPIDEMIA:\n",
      " [[314835     83]\n",
      " [   224    293]] \n",
      "\n",
      "HYPERTENSION:\n",
      " [[313473    546]\n",
      " [   399   1017]] \n",
      "\n",
      "MEDICATION:\n",
      " [[310171    818]\n",
      " [  1616   2830]] \n",
      "\n",
      "O:\n",
      " [[  7913   6301]\n",
      " [  3464 297757]] \n",
      "\n",
      "OBESE:\n",
      " [[315206     54]\n",
      " [    50    125]] \n",
      "\n",
      "SMOKER:\n",
      " [[313272    531]\n",
      " [   547   1085]] \n",
      "\n",
      "preparing gold label targets ...\n",
      "\n",
      "F1 Scores for global labels:\n",
      "ALL (average=\"micro\"): 0.9277161862527716\n",
      "CAD:\n",
      " [[2092  150]\n",
      " [ 176 1694]] \n",
      "\n",
      "DIABETES:\n",
      " [[1694  176]\n",
      " [ 150 2092]] \n",
      "\n",
      "CAD: 0.8708971553610503\n",
      "DIABETES: 0.943448275862069\n",
      "FAMILY_HIST: 0.0\n",
      "HYPERLIPIDEMIA: 0.9164882226980728\n",
      "HYPERTENSION: 0.9331713244228432\n",
      "MEDICATION: 0.9888888888888889\n",
      "OBESE: 0.893854748603352\n",
      "SMOKER: 0.9106382978723405\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "from sklearn.metrics import log_loss, roc_auc_score, accuracy_score, hamming_loss, f1_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from models import get_rnn_model\n",
    "from cm import multilabel_confusion_matrix\n",
    "from data_process import get_embedding_matrix, data_generator, get_all_notes_labels, get_features, get_targets, get_gold_label_targets\n",
    "\n",
    "# Customized Evaluation for keras model\n",
    "class CustomEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = list(validation_data)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = []\n",
    "            for x in self.X_val:\n",
    "                y = np.squeeze(self.model.predict_on_batch(x))\n",
    "                y_pred.append(y)\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            y_pred_ham = y_pred > 0.5\n",
    "            y_val = np.concatenate(self.y_val)\n",
    "            roc = roc_auc_score(y_val, y_pred, average='micro')\n",
    "            loss = log_loss(y_val, y_pred)\n",
    "            ham = hamming_loss(y_val, y_pred_ham)\n",
    "            sub = accuracy_score(y_val, y_pred_ham)\n",
    "            f1 = f1_score(y_val, y_pred_ham, average='micro')\n",
    "            print(\"Adiitional val metrics: - ROC-AUC: %.6f - Log-Loss: %.6f - Hamming-Loss: %.6f - Subset-Accuracy: %.6f - F1-Score: %.6f\" % (roc, loss, ham, sub, f1))\n",
    "            \n",
    "def model_train(param, \n",
    "                notes_train, \n",
    "                labels_train, \n",
    "                up_notes_train, \n",
    "                up_labels_train, \n",
    "                gold_labels_train, \n",
    "                notes_test, \n",
    "                labels_test, \n",
    "                gold_labels_test,\n",
    "                results_file,\n",
    "                verbose=1):\n",
    "    \n",
    "    print('*'*80)\n",
    "    print(\"Parameters (note: embed_size*10, latent_dim*64):\\n\", param)\n",
    "    print('*'*80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # assign parameters\n",
    "    up = int(param['up'])\n",
    "    window_size = int(param['window_size'])\n",
    "    embed_size = int(param['embed_size'] * 10)\n",
    "    latent_dim = int(param['latent_dim'] * 64)\n",
    "    dropout_rate = param['dropout_rate']\n",
    "    epochs = param['epochs']\n",
    "    category = param['category']\n",
    "    max_features = 60000 #param['max_features']\n",
    "    train_embed = True #param['train_embed']\n",
    "    model_type = 'CuDNNLSTM' #param['model_type']\n",
    "    \n",
    "    # upsampling\n",
    "    if up > 0:\n",
    "        if verbose != 0: print('upsampling for %d times...' % (up))\n",
    "        notes_train = [note + up * up_note for note, up_note in zip(notes_train, up_notes_train)]\n",
    "        labels_train = [label + up * up_label for label, up_label in zip(labels_train, up_labels_train)]\n",
    "    notes = notes_train + notes_test\n",
    "    labels = labels_train + labels_test\n",
    "    gold_labels = gold_labels_train + gold_labels_test\n",
    "    \n",
    "    # prepare features\n",
    "    X_train_seq, X_test_seq, word_index = get_features(max_features, notes_train, notes_test, verbose=1)\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "\n",
    "    # prepare embedding matrix\n",
    "    if train_embed:\n",
    "        if verbose != 0: print('preparing pretrained embedding matrix ...')\n",
    "        w2v = Word2Vec(notes, size=embed_size, window=window_size, min_count=1, workers=4)\n",
    "        embedding_index = dict(zip(w2v.wv.index2word, w2v.wv.vectors))\n",
    "        embedding_matrix = get_embedding_matrix(embedding_index=embedding_index, \n",
    "                                                word_index=word_index, \n",
    "                                                max_features=max_features, \n",
    "                                                embed_size=embed_size)\n",
    "        \n",
    "    # prepare targets\n",
    "    Y_train, Y_test, mlb, num_labels = get_targets(labels_train, labels_test, category, verbose=1)  \n",
    "\n",
    "    # get rnn model\n",
    "    model = get_rnn_model(nb_words=nb_words, \n",
    "                          num_labels=num_labels, \n",
    "                          embed_size=embed_size, \n",
    "                          latent_dim=latent_dim, \n",
    "                          model_type=model_type, \n",
    "                          embedding_matrix=embedding_matrix, \n",
    "                          dropout=dropout_rate, \n",
    "                          train_embed=train_embed)\n",
    "    if verbose != 0: \n",
    "        print('model summary:')\n",
    "        print(model.summary())\n",
    "    \n",
    "    # model compiling\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # model training\n",
    "    if verbose != 0: print('\\ntraining model ...')\n",
    "    custevl = CustomEvaluation(validation_data=(X_test_seq, Y_test), interval=1)\n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0005, patience=2, verbose=0, mode='auto')\n",
    "    train_gen = data_generator(X_train_seq, Y_train)\n",
    "    test_gen = data_generator(X_test_seq, Y_test)\n",
    "    v = 1 if verbose != 0 else 0  \n",
    "    hist = model.fit_generator(train_gen,\n",
    "                                steps_per_epoch=len(Y_train),\n",
    "                                epochs=epochs,\n",
    "                                validation_data=test_gen,\n",
    "                                validation_steps=len(Y_test),\n",
    "                                callbacks=[custevl, earlystop],\n",
    "                                verbose=v)\n",
    "\n",
    "    # prediction of test data\n",
    "    if verbose != 0: print('predicting test data ...')\n",
    "    Y_pred = []\n",
    "    for x in X_test_seq:\n",
    "        x = np.array(x).reshape((1,-1))\n",
    "        y_pred = np.squeeze(model.predict_on_batch(x))\n",
    "        Y_pred.append(y_pred)\n",
    "    Y_pred_concat = np.concatenate(Y_pred)\n",
    "    Y_val = np.concatenate(Y_test)\n",
    "\n",
    "    # confusion matrix \n",
    "    if verbose == 2: \n",
    "        cm = multilabel_confusion_matrix(Y_val, np.where(Y_pred_concat > 0.5, 1, 0))\n",
    "        for i, j in zip(cm, mlb.classes_):\n",
    "            print(j+':\\n', i,'\\n')\n",
    "\n",
    "    # prepare gold label targets\n",
    "    Y_gold_test, Y_gold_pred, gmlb = get_gold_label_targets(Y_pred, gold_labels, gold_labels_test, mlb, category=category, verbose=1) \n",
    "\n",
    "    # f1 scores for gold label\n",
    "    f1 = f1_score(Y_gold_test, Y_gold_pred, average='micro')\n",
    "    print('\\nF1 Scores for global labels:\\nALL (average=\"micro\"):', f1)\n",
    "    \n",
    "    # confusion matrix for gold label\n",
    "    if verbose == 2: \n",
    "        gcm = multilabel_confusion_matrix(np.concatenate(Y_gold_test), np.concatenate(Y_gold_pred))\n",
    "        for i, j in zip(gcm, gmlb.classes_):\n",
    "            print(j+':\\n', i,'\\n')\n",
    "    \n",
    "    # f1 score list\n",
    "    if verbose == 2: \n",
    "        f1_all = f1_score(Y_gold_test, Y_gold_pred, average=None)\n",
    "        for i, j in zip(f1_all, gmlb.classes_):\n",
    "            print(j+': '+str(i))\n",
    "    \n",
    "    print('\\n')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # save results\n",
    "    with open(results_file,\"a\") as f:\n",
    "        f.write(\"Parameters (note: embed_size*10, ltent_dim*64):\\n\" + str(param))\n",
    "        f.write('\\nF1 Scores for global labels(average=\"micro\"): %.3f; Running time: %.1f\\n' % (f1, elapsed_time))\n",
    "          \n",
    "    return f1\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # loading data \n",
    "    if os.path.exists('loaded_data.dat'):\n",
    "        \n",
    "        with open('loaded_data.dat','rb') as f:\n",
    "            notes_train = pickle.load(f)\n",
    "            labels_train = pickle.load(f)\n",
    "            up_notes_train = pickle.load(f)\n",
    "            up_labels_train = pickle.load(f)\n",
    "            gold_labels_train = pickle.load(f)\n",
    "            notes_test = pickle.load(f)\n",
    "            labels_test = pickle.load(f)\n",
    "            gold_labels_test = pickle.load(f)\n",
    "            \n",
    "    else:\n",
    "        \n",
    "        notes_train_1, labels_train_1, up_notes_train_1, up_labels_train_1, gold_labels_train_1 = get_all_notes_labels('/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set1') \n",
    "        notes_train_2, labels_train_2, up_notes_train_2, up_labels_train_2, gold_labels_train_2 = get_all_notes_labels('/host_home/data/i2b2/2014/training/training-RiskFactors-Complete-Set2') \n",
    "\n",
    "        notes_train = notes_train_1 + notes_train_2\n",
    "        labels_train = labels_train_1 + labels_train_2\n",
    "        up_notes_train = up_notes_train_1 + up_notes_train_2\n",
    "        up_labels_train = up_labels_train_1 + up_labels_train_2\n",
    "        gold_labels_train = gold_labels_train_1 + gold_labels_train_2\n",
    "\n",
    "        notes_test, labels_test, _1, _2, gold_labels_test = get_all_notes_labels('/host_home/data/i2b2/2014/testing/testing-RiskFactors-Complete')\n",
    "\n",
    "        with open('loaded_data.dat','wb') as f:\n",
    "            pickle.dump(notes_train, f)\n",
    "            pickle.dump(labels_train, f)\n",
    "            pickle.dump(up_notes_train, f)\n",
    "            pickle.dump(up_labels_train, f)\n",
    "            pickle.dump(gold_labels_train, f)\n",
    "            pickle.dump(notes_test, f)\n",
    "            pickle.dump(labels_test, f)\n",
    "            pickle.dump(gold_labels_test, f)\n",
    "\n",
    "    # loading parameters space\n",
    "    param = {'up': 5, 'window_size': 3, 'embed_size': 5, 'latent_dim': 5, 'dropout_rate': 0.0, 'epochs': 20, 'category': 'cat_only'}\n",
    "    \n",
    "    results_file = \"demo_results_\" + time.strftime(\"%Y%m%d\") + \".txt\"\n",
    "    \n",
    "    model_train(param, \n",
    "                notes_train, \n",
    "                labels_train, \n",
    "                up_notes_train, \n",
    "                up_labels_train, \n",
    "                gold_labels_train, \n",
    "                notes_test, \n",
    "                labels_test, \n",
    "                gold_labels_test,\n",
    "                results_file=results_file,\n",
    "                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
